{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    ['Pointy', 'Round',     'Present', 1],\n",
    "    ['Floppy', 'Not round', 'Present', 1],\n",
    "    ['Floppy', 'Round',     'Absent',  0],\n",
    "    ['Pointy', 'Not round', 'Present', 0],\n",
    "    ['Pointy', 'Round',     'Present', 1],\n",
    "    ['Pointy', 'Round',     'Absent',  1],\n",
    "    ['Floppy', 'Not round', 'Absent',  0],\n",
    "    ['Pointy', 'Round',     'Absent',  1],\n",
    "    ['Floppy', 'Round',     'Absent',  0],\n",
    "    ['Floppy', 'Round',     'Absent',  0],\n",
    "])\n",
    "\n",
    "X, Y = data[:, :-1], data[:, -1]\n",
    "Y = Y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data = None, children = None, split_on = None, pred_class = None, is_leaf = False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: numpy.ndarray, default=None\n",
    "            Stores corresponding data in each node.\n",
    "        children: dict(feat_value: Node), default=None\n",
    "            Contains all direct nodes after data is split into nodes.\n",
    "        split_on: int, default=None\n",
    "            Shows which feature, the model decides to split.\n",
    "        pred_class : str, default=None\n",
    "            The predicted class for the node (only applicable to leaf nodes)\n",
    "        is_leaf: bool, default=False\n",
    "            Demonstrates whether the node is a leaf or not!\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "    \n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_entropy(Y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y: numpy.ndarray\n",
    "            The labels array.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        entropy: flaot\n",
    "            The entropy value of the given labels.\n",
    "        \"\"\"\n",
    "        _, labels_count = np.unique(Y, return_counts=True)\n",
    "        total_instances = len(Y)\n",
    "\n",
    "        entropy = sum([-label_count/total_instances * np.log2(label_count/total_instances) for label_count in labels_count])\n",
    "        return entropy\n",
    "    \n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        \"\"\"\n",
    "        Split the dataset based on a specific feature index.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: numpy.ndarray\n",
    "            The dataset to be split.\n",
    "        feat_index: int\n",
    "            The index of the feature to perform the split.\n",
    "        Returns:\n",
    "        -----------\n",
    "        - split_nodes: dict\n",
    "            A dictionary of split nodes. \n",
    "            (feature value as key, corresponding node as value)\n",
    "        - weighted_entropy: float\n",
    "            The weighted entropy of the split.\n",
    "        \"\"\"\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "        total_instances = len(data)\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            node = Node(data = partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = self.get_y(partition)\n",
    "            node_entropy = self.calculate_entropy(partition_y)\n",
    "            weighted_entropy += (len(partition)/total_instances) * node_entropy\n",
    "\n",
    "        return split_nodes, weighted_entropy\n",
    "    \n",
    "    def best_split(self, node):\n",
    "        \"\"\"\n",
    "        Find the best split for the given node.\n",
    "        (data in node.data)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        node: Node\n",
    "            The node for which the best split is being determined.\n",
    "\n",
    "        If the node meets the criteria to stop splitting:\n",
    "            - Mark the node as a leaf.\n",
    "            - Assign a predicted class for future predictions based on the target values (y).\n",
    "            - return.\n",
    "\n",
    "        Otherwise:\n",
    "            - Initialize variables for tracking the best split.\n",
    "            - Iterate over the features to find the best split.\n",
    "            - Split the data based on each feature and calculate the weighted entropy of the split.\n",
    "            - Compare the current weighted entropy with the previous best entropy.\n",
    "            - Update the best split variables if the current split has lower entropy.\n",
    "            - update the node with the best split information, including child nodes and the feature index used for the split.\n",
    "            - Recursively call the best_split function for each child node.\n",
    "        \"\"\"\n",
    "        # Check if the node meets stopping criteria\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "\n",
    "            return \n",
    "        # Initialize variables to track the best split\n",
    "        index_feature_split = -1\n",
    "        min_entropy = 1\n",
    "\n",
    "        # Iterate over all features (ignore y)\n",
    "        for i in range(data.shape[1] - 1):\n",
    "            split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n",
    "            if weighted_entropy < min_entropy:\n",
    "                child_nodes, min_entropy = split_nodes, weighted_entropy\n",
    "                index_feature_split = i\n",
    "\n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "\n",
    "        # Recursively call the best_split function for each child node\n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "        \n",
    "    def meet_criteria(self, node):\n",
    "        \"\"\"\n",
    "        Check if the criteria for stopping the tree expansion is met for a given node. Here we only check if the entropy of the target values (y) is zero.\n",
    "        Additionally, you can customize criteria based on your specific requirements. For instance, you can set the maximum depth for the decision tree or incorporate other conditions for stopping the tree expansion. Modify the implementation of this method according to your desired criteria.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        node : Node\n",
    "            The node to check for meeting the stopping criteria.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        bool\n",
    "            True if the criteria is met, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 else False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        \"\"\"\n",
    "        Get the target (y) from the data.\n",
    "        \"\"\"\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        \"\"\"\n",
    "        Get the predicted class label based on the majority vote.\n",
    "        \"\"\"\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "\n",
    "        return labels[index]\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "        return predictions\n",
    "    \n",
    "    def traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Recursively traverse the decision tree to predict the class label for a given input.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x:\n",
    "            The input for which to make a prediction.\n",
    "\n",
    "        node:\n",
    "            The current node being traversed in the decision tree.\n",
    "        \"\"\"\n",
    "\n",
    "        # check if the current node is a leaf node\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "        \n",
    "        # get the feature value at the split point for the current node\n",
    "        feat_value = x[node.split_on]\n",
    "\n",
    "        # Recursively traverse the decision tree using the child node corresponding to the feature value\n",
    "        predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "        return predicted_class\n",
    "    \n",
    "\n",
    "    # share github\n",
    "    # interpretability and explainability\n",
    "    # non-parametric\n",
    "    # inference time\n",
    "    # general overview of the decision trees\n",
    "    # gradient boosting implement\n",
    "    #  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '0'], dtype='<U1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "model.predict(\n",
    "    [['Pointy', 'Round',     'Present'],\n",
    "    ['Floppy', 'Not round', 'Present'],\n",
    "    ['Floppy', 'Round',     'Absent']\n",
    "]) # ['1' '1' '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root\n",
      "- split on: Ear Shape\n",
      "- children: {np.str_('Floppy'): <__main__.Node object at 0x1668a3ee0>, np.str_('Pointy'): <__main__.Node object at 0x166892bb0>} \n",
      "\n",
      "Pointy Node\n",
      "- split on: Face Shape\n",
      "- children: {np.str_('Not round'): <__main__.Node object at 0x165f97d90>, np.str_('Round'): <__main__.Node object at 0x165f97d00>}\n",
      "- data: \n",
      " [['Pointy' 'Round' 'Present' '1']\n",
      " ['Pointy' 'Not round' 'Present' '0']\n",
      " ['Pointy' 'Round' 'Present' '1']\n",
      " ['Pointy' 'Round' 'Absent' '1']\n",
      " ['Pointy' 'Round' 'Absent' '1']] \n",
      "\n",
      "Floppy Node\n",
      "- split on: Whiskers\n",
      "- children: {np.str_('Absent'): <__main__.Node object at 0x1669a2a00>, np.str_('Present'): <__main__.Node object at 0x165f97d30>}\n",
      "- data: \n",
      " [['Floppy' 'Not round' 'Present' '1']\n",
      " ['Floppy' 'Round' 'Absent' '0']\n",
      " ['Floppy' 'Not round' 'Absent' '0']\n",
      " ['Floppy' 'Round' 'Absent' '0']\n",
      " ['Floppy' 'Round' 'Absent' '0']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_to_name = {0: 'Ear Shape', 1: 'Face Shape', 2: 'Whiskers'}\n",
    "\n",
    "root = model.root\n",
    "print(\"Root\")\n",
    "# Root\n",
    "print(f\"- split on: {index_to_name[root.split_on]}\" )\n",
    "# - split on: Ear Shape\n",
    "print(f\"- children: {root.children}\", '\\n')          \n",
    "# - children: {'Floppy': <__main__.Node object at 0x7f87c94d0760>, 'Pointy': <__main__.Node object at 0x7f87c94d01c0>} \n",
    "\n",
    "pointy_node = root.children['Pointy']\n",
    "print(\"Pointy Node\")\n",
    "# Pointy Node\n",
    "print(f\"- split on: {index_to_name[pointy_node.split_on]}\" )\n",
    "# - split on: Face Shape\n",
    "print(f\"- children: {pointy_node.children}\")\n",
    "# - children: {'Not round': <__main__.Node object at 0x7f87c94d07f0>, 'Round': <__main__.Node object at 0x7f87c94d0e80>}\n",
    "print(f\"- data: \\n {pointy_node.data}\", '\\n')\n",
    "# All pointy data\n",
    "\n",
    "floppy_node = root.children['Floppy']\n",
    "print(\"Floppy Node\")\n",
    "# Floppy Node\n",
    "print(f\"- split on: {index_to_name[floppy_node.split_on]}\" )\n",
    "# - split on: Whiskers\n",
    "print(f\"- children: {floppy_node.children}\")\n",
    "# - children: {'Absent': <__main__.Node object at 0x7f87c94d09d0>, 'Present': <__main__.Node object at 0x7f87c94d0400>}\n",
    "print(f\"- data: \\n {floppy_node.data}\", '\\n')\n",
    "# All floppy data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
